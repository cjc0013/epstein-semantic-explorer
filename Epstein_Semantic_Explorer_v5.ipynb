{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb31e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#  EPSTEIN SEMANTIC EXPLORER v5  â€” Reporter Power Edition\n",
    "# ======================================================================\n",
    "#  Features:\n",
    "#   âœ” Auto file detection + upload\n",
    "#   âœ” Cluster viewer\n",
    "#   âœ” Keyword search (BM25-lite)\n",
    "#   âœ” Cluster summaries\n",
    "#   âœ” Topic pseudo-modeling (centroid-based, upgraded)\n",
    "#   âœ” Entity extraction (People / Orgs / Places)\n",
    "#   âœ” Timeline builder\n",
    "#   âœ” Cluster similarity heatmap (text-based centroids)\n",
    "#   âœ” Cross-cluster entity search\n",
    "#\n",
    "#  Requirements: ONLY a JSONL file with fields:\n",
    "#         {\"id\": \"...\", \"cluster\": 96, \"text\": \"...\"}\n",
    "#\n",
    "#  NO external models required. Pure Python.\n",
    "# ======================================================================\n",
    "\n",
    "import os, json, re, math\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from google.colab import files\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "TARGET_NAME = \"epstein_semantic.jsonl\"\n",
    "\n",
    "print(\"ðŸ” Checking for existing .jsonl filesâ€¦\")\n",
    "\n",
    "available_jsonl = [f for f in os.listdir(\"/content\") if f.endswith(\".jsonl\")]\n",
    "\n",
    "if TARGET_NAME in available_jsonl:\n",
    "    print(f\"âœ” Using existing {TARGET_NAME}\")\n",
    "else:\n",
    "    if available_jsonl:\n",
    "        print(f\"âœ” Found other JSONL files: {available_jsonl}\")\n",
    "        TARGET_NAME = available_jsonl[0]\n",
    "        print(f\"âœ” Using: {TARGET_NAME}\")\n",
    "    else:\n",
    "        print(\"âš  No JSONL found â€” please upload epstein_semantic.jsonl\")\n",
    "        uploaded = files.upload()\n",
    "        names = list(uploaded.keys())\n",
    "        if not names:\n",
    "            raise RuntimeError(\"âŒ No file uploaded.\")\n",
    "        TARGET_NAME = names[0]\n",
    "        print(f\"âœ” Loaded: {TARGET_NAME}\")\n",
    "\n",
    "records = []\n",
    "with open(TARGET_NAME, \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            records.append(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"ðŸ“¦ Loaded {len(records)} records\")\n",
    "\n",
    "cluster_map = defaultdict(list)\n",
    "all_texts = []\n",
    "for r in records:\n",
    "    cid = r.get(\"cluster\", -1)\n",
    "    cluster_map[cid].append(r)\n",
    "    all_texts.append(r.get(\"text\",\"\"))\n",
    "\n",
    "clusters = sorted(cluster_map.keys())\n",
    "print(\"ðŸ§© Clusters:\", clusters)\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
    "\n",
    "def text_vector(text):\n",
    "    toks = tokenize(text)\n",
    "    counts = Counter(toks)\n",
    "    return counts\n",
    "\n",
    "def centroid(docs):\n",
    "    C = Counter()\n",
    "    for d in docs:\n",
    "        C.update(text_vector(d[\"text\"]))\n",
    "    return C\n",
    "\n",
    "def cosine(a, b):\n",
    "    num = 0\n",
    "    da = 0\n",
    "    db = 0\n",
    "    for k in set(a.keys()) | set(b.keys()):\n",
    "        va = a.get(k,0)\n",
    "        vb = b.get(k,0)\n",
    "        num += va*vb\n",
    "        da += va*va\n",
    "        db += vb*vb\n",
    "    if da == 0 or db == 0:\n",
    "        return 0\n",
    "    return num / math.sqrt(da*db)\n",
    "\n",
    "def view_cluster(cid, max_chars=2000):\n",
    "    cid = int(cid)\n",
    "    if cid not in cluster_map:\n",
    "        print(\"âŒ Cluster not found.\")\n",
    "        return\n",
    "\n",
    "    docs = cluster_map[cid]\n",
    "    print(f\"\\n=== CLUSTER {cid} ({len(docs)} docs) ===\\n\")\n",
    "\n",
    "    for i, d in enumerate(docs):\n",
    "        text = d.get(\"text\",\"\").strip()\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \" â€¦ [truncated]\"\n",
    "        print(f\"\\n--- Doc {i+1}/{len(docs)} id={d.get('id')} ---\\n{text}\\n\")\n",
    "\n",
    "cid_box = widgets.Text(description=\"Cluster #\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_submit(change):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        view_cluster(change[\"new\"])\n",
    "\n",
    "cid_box.observe(on_submit, names=\"value\")\n",
    "\n",
    "print(\"\\nâž¡ Enter cluster number below:\")\n",
    "display(cid_box, out)\n",
    "\n",
    "print(\"\\nðŸ” Initializing BM25-lite structures...\")\n",
    "\n",
    "docs_text = [r[\"text\"] for r in records]\n",
    "tokenized_docs = [tokenize(t) for t in docs_text]\n",
    "doc_freq = Counter()\n",
    "\n",
    "for toks in tokenized_docs:\n",
    "    for t in set(toks):\n",
    "        doc_freq[t] += 1\n",
    "\n",
    "Ndocs = len(records)\n",
    "avg_len = sum(len(t) for t in tokenized_docs)/Ndocs\n",
    "\n",
    "def bm25_score(query, doc_toks):\n",
    "    k=1.5; b=0.75\n",
    "    score = 0\n",
    "    q_toks = tokenize(query)\n",
    "    for q in q_toks:\n",
    "        df = doc_freq.get(q,0)\n",
    "        if df == 0:\n",
    "            continue\n",
    "        idf = math.log((Ndocs - df + 0.5)/(df + 0.5) + 1)\n",
    "        tf = doc_toks.count(q)\n",
    "        denom = tf + k*(1 - b + b*(len(doc_toks)/avg_len))\n",
    "        score += idf*(tf*(k+1))/denom\n",
    "    return score\n",
    "\n",
    "def search(query, top_k=10):\n",
    "    scores = []\n",
    "    for r, toks in zip(records, tokenized_docs):\n",
    "        s = bm25_score(query, toks)\n",
    "        if s > 0:\n",
    "            scores.append((s, r))\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    print(f\"\\n=== Search results for '{query}' ===\")\n",
    "    for s, r in scores[:top_k]:\n",
    "        cid = r.get(\"cluster\")\n",
    "        print(f\"\\nScore {s:.2f} â€” Cluster {cid} â€” id={r.get('id')}\")\n",
    "        print(r[\"text\"][:400], \"...\\n\")\n",
    "\n",
    "def summarize_cluster(cid):\n",
    "    cid = int(cid)\n",
    "    if cid not in cluster_map:\n",
    "        print(\"âŒ Cluster not found.\")\n",
    "        return\n",
    "\n",
    "    docs = cluster_map[cid][:5]\n",
    "    print(f\"\\nðŸ“ Summary for Cluster {cid}\\n\")\n",
    "\n",
    "    for d in docs:\n",
    "        snippet = d[\"text\"].strip().split(\"\\n\")[0]\n",
    "        print(\"â€¢\", snippet[:200], \"\\n\")\n",
    "\n",
    "print(\"ðŸ§  Computing cluster centroids...\")\n",
    "\n",
    "cluster_centroids = {}\n",
    "for cid in clusters:\n",
    "    cluster_centroids[cid] = centroid(cluster_map[cid])\n",
    "\n",
    "STOPWORDS = set(\"\"\"\n",
    "the and to of a in is this that for on with as be or by from at\n",
    "an it are was you your if but have we they his her she their our\n",
    "subject re fw message communication thereof all may any doc email\n",
    "\"\"\".split())\n",
    "\n",
    "def top_terms(centroid, n=6):\n",
    "    filtered = {w:c for w,c in centroid.items()\n",
    "                if w not in STOPWORDS and len(w) > 2 and c > 1}\n",
    "    if not filtered:\n",
    "        return [\"(no meaningful terms)\"]\n",
    "    return [w for w,_ in Counter(filtered).most_common(n)]\n",
    "\n",
    "def show_topics():\n",
    "    print(\"\\n=== Improved Pseudo-Topics by Cluster ===\")\n",
    "    for cid in clusters:\n",
    "        terms = top_terms(cluster_centroids[cid])\n",
    "        print(f\"Cluster {cid:<4} | {' '.join(terms)}\")\n",
    "\n",
    "name_re = re.compile(r\"\\b[A-Z][a-z]+(?: [A-Z][a-z]+){0,2}\\b\")\n",
    "date_re = re.compile(r\"(19|20)\\d\\d[-/\\.]([0][1-9]|1[0-2])[-/\\.]([0][1-9]|[12]\\d|3[01])\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    return name_re.findall(text)\n",
    "\n",
    "def cluster_entities(cid):\n",
    "    cid = int(cid)\n",
    "    ents = Counter()\n",
    "    for d in cluster_map[cid]:\n",
    "        for e in extract_entities(d[\"text\"]):\n",
    "            ents[e] += 1\n",
    "    print(f\"\\nðŸ‘¤ Entities in Cluster {cid}\")\n",
    "    for e,c in ents.most_common(20):\n",
    "        print(f\"{e:20} {c}\")\n",
    "\n",
    "def show_timeline():\n",
    "    timeline = []\n",
    "    for r in records:\n",
    "        text = r[\"text\"]\n",
    "        dates = date_re.findall(text)\n",
    "        for d in dates:\n",
    "            dt = \"-\".join(d)\n",
    "            timeline.append((dt, text[:100]))\n",
    "\n",
    "    timeline.sort()\n",
    "    print(\"\\nðŸ•° Timeline Extracted:\")\n",
    "    for dt, snip in timeline[:100]:\n",
    "        print(dt, \"-\", snip)\n",
    "\n",
    "def cluster_similarity():\n",
    "    print(\"\\nðŸ“Š Cluster Similarity Matrix (0â€“1 cosine)\\n\")\n",
    "    cids = clusters\n",
    "    vecs = [cluster_centroids[c] for c in cids]\n",
    "\n",
    "    for i,cid_i in enumerate(cids):\n",
    "        row = []\n",
    "        for j,cid_j in enumerate(cids):\n",
    "            sim = cosine(vecs[i], vecs[j])\n",
    "            row.append(f\"{sim:0.2f}\")\n",
    "        print(f\"{cid_i:<4} | \" + \"  \".join(row))\n",
    "\n",
    "def entity_to_clusters(entity, top_k=10):\n",
    "    hits = []\n",
    "    for cid in clusters:\n",
    "        count = sum(entity.lower() in d[\"text\"].lower() for d in cluster_map[cid])\n",
    "        if count:\n",
    "            hits.append((count,cid))\n",
    "    hits.sort(reverse=True)\n",
    "    print(f\"\\nðŸ”Ž Entity '{entity}' appears in:\")\n",
    "    for count,cid in hits[:top_k]:\n",
    "        print(f\"Cluster {cid}: {count} hits\")\n",
    "\n",
    "print(\"\\nâœ… Epstein Semantic Explorer v5 loaded and ready.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
